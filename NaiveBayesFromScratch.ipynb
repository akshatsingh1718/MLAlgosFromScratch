{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier form scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a very popular Supervised learning technique. It is very simple but very powerful algorithm which works very well with large datasets and sparse matrices, like preprocessed text data which creates thousand of vectors depending on the number of words in dictionary.It works really well with text data projects like sentiment data analysis, performs good with document categorization projects, and also it is great in predicting categorical data in projects such as email spam classification.\n",
    "\n",
    "<img src='http://shatterline.com/blog/wp-content/uploads/2013/09/bayes-pictorial5.png' height='500px' width='900px'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes():\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        # calculating the mean, variance and prior P(H) for each class\n",
    "        for i, c in enumerate(self._classes):\n",
    "            X_for_class_c = X[y==c]\n",
    "            self._mean[i, :] = X_for_class_c.mean(axis=0)\n",
    "            self._var[i, :] = X_for_class_c.var(axis=0)\n",
    "            # Prior = no of class occurs in y/ total no of samples\n",
    "            self._priors[i] = X_for_class_c.shape[0]/float(n_samples)\n",
    "            \n",
    "    \n",
    "    def _calculate_likelihood(self, class_idx, x):\n",
    "        '''\n",
    "        Function to calculate likelihood, P(X | Class_j)\n",
    "        of data X given the mean and variance\n",
    "        '''\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        num = np.exp(-(x-mean)**2 / (2*var))\n",
    "        denom = np.sqrt(2 * np.pi * var)\n",
    "        return num/denom\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._classify_sample(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    \n",
    "    def _classify_sample(self, x):\n",
    "        posteriors = []\n",
    "        # calculating posteriors probability for each class\n",
    "        for i, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[i])\n",
    "            posterior = np.sum(np.log(self._calculate_likelihood(i, x)))\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "            # return the class with highest posterior probability\n",
    "        return self._classes[np.argmax(posteriors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# splitting data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model and comparing it with sklearn's NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy NB accuracy: 0.796\n",
      "Finished in 0.026 second(s)\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "nb = GaussianNaiveBayes()\n",
    "nb.fit(X_train, y_train)\n",
    "predictions = nb.predict(X_test)\n",
    "end = time.perf_counter()\n",
    "print(f'Numpy NB accuracy: {accuracy_score(y_test, predictions)}')\n",
    "print(f'Finished in {round(end-start, 3)} second(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn Naive Bayes accuracy: 0.796\n",
      "Finished in 0.004 second(s)\n"
     ]
    }
   ],
   "source": [
    " from sklearn.naive_bayes import GaussianNB\n",
    " start = time.perf_counter()\n",
    " sk_nb = GaussianNB()\n",
    " sk_nb.fit(X_train, y_train).predict(X_test)\n",
    " sk_predictions = sk_nb.predict(X_test)\n",
    " end = time.perf_counter()\n",
    " print(f\"scikit-learn Naive Bayes accuracy: {accuracy_score(y_test, sk_predictions)}\")\n",
    " print(f'Finished in {round(end-start, 3)} second(s)')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
